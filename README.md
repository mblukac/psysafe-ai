<div align="center">
    <h1>
    PsySafe AI
    </h1>
    <p>
    An open-source initiative dedicated to enhancing psychological safety in AI language model applications<br>
    </p>
    <p>
    <img src="assets/imgs/psysafe_capybara.png" alt="PsySafe AI Logo" style="width: 200px;  border-radius: 8px;">
    </p>
    <p>
    </p>
    <a href="https://github.com/mblukac/psysafe-ai"><img src="https://img.shields.io/badge/Python-3.8+-orange" alt="version"></a>
    <a href="https://github.com/mblukac/psysafe-ai"><img src="https://img.shields.io/badge/License-MIT-red.svg" alt="mit"></a>
</div>

# Description
â€‹PsySafe AI is an open-source initiative dedicated to enhancing psychological safety in AI language model applications. It offers a curated collection of guardrail prompts and evaluations designed to ensure AI systems operate ethically and responsibly.

# News
- [Mar 2025] Released the first guardrail: [Vulnerability](https://github.com/mblukac/psysafe-ai/tree/main/guardrails/vulnerability)! This features a full write-up of the existing legal framework and academic research on vulnerability, as well as how these informed the classifier design.
- [Mar 2025] ðŸ”¥ Launch and started building

# Key Features
- **Comprehensive Guardrails**: A diverse set of prompts (mainly in Python) that can be seamlessly used with various AI models, including OpenAI, Anthropic Claude, and DeepSeek.â€‹
- **Ethical Compliance**: Ensures AI assistants avoid generating harmful content and respect user privacy.â€‹
- **User Well-being Focus**: Equips AI systems to recognize and appropriately respond to user distress, such as mental health crises, within ethical boundaries.â€‹
- **Framework Agnostic**: Designed for compatibility across different AI platforms, facilitating easy implementation with minimal setup.

## Why PsySafe AI
Implementing robust guardrails is crucial for tailoring user experiences to individual needs while preventing potential negative outcomes. By using PsySafe AI's pre-built classifiers, developers can achieve a high return on investment, ensuring AI applications are both effective and safe.â€‹

**Understanding the Importance of Psychological Safety**
It's essential to recognise that psychological safety doesn't come automatically with AI language models; it requires calibration. Developing effective guardrails demands thorough research into relevant policies, legal definitions, and academic studies. **PsySafe AI** undertakes this groundwork, providing developers with well-informed tools to:â€‹

- **Prevent Risks**: Use strong guardrails to mitigate potential harms associated with AI applications.â€‹
- **Make Informed Adjustments**: Empower developers to refine guardrail prompts based on comprehensive materials and research.

