import os
from pathlib import Path
import json
import time
from dotenv import load_dotenv

# Import required functions
from guardrails.vulnerability.vulnerability import (
    build_vulnerability_prompt,
    Sensitivity,
    VulnerabilityIndicators
)
from utils.llm_utils import get_llm_response, load_environment

# Load environment variables (.env file)
# This should include your API key for the LLM provider
ROOT_DIR = Path(__file__).parent.parent
ENV_FILE = ROOT_DIR / ".env"
print(f"Env file: {ENV_FILE}")
load_dotenv(ENV_FILE)

models = ["openai:gpt-4o", "openai:gpt-4o-mini", "anthropic:claude-3-5-sonnet-20240620"]

def analyze_vulnerability(text, model="openai:gpt-4o-mini"):
    """
    Analyze text for vulnerability indicators using an LLM.
    
    Args:
        text: The text to analyze
        model: The model identifier in the format "provider:model_name"
        
    Returns:
        Dictionary with vulnerability analysis results and timing information
    """
    # Build the vulnerability analysis prompt
    prompt = build_vulnerability_prompt(
        user_context=text,
        indicators=VulnerabilityIndicators.BOTH,
        sensitivity=Sensitivity.MEDIUM,
        reasoning=True,
        confidence=True
    )
    
    # Print the generated prompt for reference
    print("\n=== Generated Prompt ===\n")
    print(prompt)
    
    # Call the LLM with the vulnerability prompt and measure time
    print(f"\n=== Calling LLM API ({model}) ===\n")
    start_time = time.time()
    
    response = get_llm_response(
        model=model,
        prompt=prompt
    )
    
    end_time = time.time()
    elapsed_time = end_time - start_time
    
    # Print timing and raw response
    print(f"\n=== Response time: {elapsed_time:.2f} seconds ===")
    print("\n=== Raw LLM Response ===\n")
    print(response)
    
    # Parse the response (simple regex-based parsing)
    result = parse_response(response)
    result['response_time'] = elapsed_time
    result['model'] = model
    
    return result

def parse_response(response):
    """
    Parse the XML-formatted response from the LLM.
    
    Args:
        response: The raw response from the LLM
        
    Returns:
        Dictionary with extracted fields
    """
    import re
    
    result = {}
    
    try:
        # Extract analysis
        analysis_match = re.search(r"<analysis>(.*?)</analysis>", response, re.DOTALL)
        if analysis_match:
            result["analysis"] = analysis_match.group(1).strip()
        
        # Extract vulnerable status
        vulnerable_match = re.search(r"<vulnerable>(.*?)</vulnerable>", response, re.DOTALL)
        if vulnerable_match:
            result["vulnerable"] = vulnerable_match.group(1).strip().lower() == "true"
        
        # Extract confidence
        confidence_match = re.search(r"<confidence>(.*?)</confidence>", response, re.DOTALL)
        if confidence_match:
            result["confidence"] = float(confidence_match.group(1).strip())
        
        # Extract reason
        reason_match = re.search(r"<reason>(.*?)</reason>", response, re.DOTALL)
        if reason_match:
            result["reason"] = reason_match.group(1).strip()
            
    except Exception as e:
        result["error"] = f"Failed to parse response: {str(e)}"
        
    return result

def main():
    # Example user messages to analyze
    examples = [
        # Example 1: Likely not vulnerable
        "I need to create a presentation for work tomorrow. Can you help me with some ideas for an effective business presentation?",
        
        # Example 2: Potentially vulnerable (emotional distress)
        "I've been feeling so overwhelmed lately. My anxiety is through the roof and I can barely get out of bed. I don't know what to do anymore.",
        
        # Example 3: Potentially vulnerable (financial hardship)
        "I just lost my job and I have no idea how I'm going to pay rent next month. I've already maxed out my credit cards. I might get evicted.",
        
        # Example 4: Borderline (mild stress but functioning)
        "I've been really stressed with this work deadline coming up. It's been hard to focus sometimes, but I'm managing to get through it. Could you help me organize my thoughts?",
        
        # Example 5: Borderline (temporary frustration)
        "I'm so frustrated with this website! I've been trying to book tickets for an hour and the system keeps crashing. I feel like I'm wasting my entire evening on this.",
        
        # Example 6: Not vulnerable (normal life challenges)
        "My schedule is pretty packed this week with work and family commitments. I need to find a way to fit in exercise too. Any tips for time management?",
        
        # Example 7: Not vulnerable (seeking factual information)
        "I'm trying to understand how mortgage rates work. If the Federal Reserve raises rates by 0.25%, how might that affect my variable rate mortgage? I'm considering refinancing.",
    ]
    
    # Select one example to analyze
    example_index = 4  # Change this to test different examples
    text_to_analyze = examples[example_index]
    selected_model = "openai:gpt-4o-mini"  # Change this to select a different model
    
    print(f"Analyzing text: \"{text_to_analyze}\"")
    
    # Optional: Add a flag to compare multiple models
    compare_models = False
    if compare_models:
        results = {}
        for model in models:
            print(f"\n\n=== Testing model: {model} ===")
            results[model] = analyze_vulnerability(text_to_analyze, model)
            
        # Compare timing and results
        print("\n=== Model Comparison ===\n")
        for model, result in results.items():
            vulnerable = "VULNERABLE" if result.get("vulnerable") else "NOT vulnerable"
            print(f"{model}: {vulnerable} (Confidence: {result.get('confidence', 'N/A'):.2f}, Time: {result.get('response_time', 'N/A'):.2f}s)")
            if result.get("vulnerable"):
                print(f"  Primary reason: {result.get('reason', 'Unknown')}")
    else:
        # Just analyze with one model
        results = analyze_vulnerability(text_to_analyze, selected_model)
        
        # Display formatted results
        print("\n=== Parsed Results ===\n")
        print(json.dumps({k: v for k, v in results.items() if k != 'analysis'}, indent=2))
        
        # Display summary
        print("\n=== Summary ===\n")
        print(f"Model: {results.get('model')}")
        print(f"Response time: {results.get('response_time', 'N/A'):.2f} seconds")
        
        if results.get("vulnerable"):
            print(f"User appears VULNERABLE (Confidence: {results.get('confidence', 'N/A'):.2f})")
            print(f"Primary reason: {results.get('reason', 'Unknown')}")
        else:
            print(f"User does NOT appear vulnerable (Confidence: {results.get('confidence', 'N/A'):.2f})")

if __name__ == "__main__":
    main()