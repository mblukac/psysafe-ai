# Vulnerability Classification in AI Chat Interactions

## Executive Summary

Vulnerability classification in AI chat interactions involves detecting when a user may be at a heightened risk of harm or undue influence during a conversation. This report provides a detailed overview of how vulnerability is defined in both regulatory and academic contexts, why certain conversational cues are critical for detecting a vulnerable user, and how an AI-based classifier can be tuned for different sensitivity levels. Key regulatory bodies — including the UK’s FCA [1], the EU’s GDPR [2] and AI Act [3], and the US FTC [4] — recognise that some individuals (e.g. those with particular personal circumstances or diminished capacities) are especially susceptible to harm. Academic research further expands this concept to digital vulnerability, noting that the online environment itself can create new vulnerabilities even for otherwise capable users [5].

We justify a classifier design that monitors indicators such as emotional distress, cognitive confusion, and low digital literacy, as these often correlate with a user’s inability to protect their interests. For instance, regulators note that poor mental health or low literacy can increase one’s risk of harm [1], and scammers commonly exploit fear, trust, or lack of digital know-how to manipulate people [6]. To adapt the classifier to various use-cases, we propose a flexible sensitivity schema (low, medium, high) that balances thoroughness of detection against false alarms.

## Definition of Vulnerability

### Regulatory Perspectives

Regulatory agencies and laws offer complementary definitions of vulnerability in the context of consumers and data subjects. The UK Financial Conduct Authority (FCA) defines a vulnerable customer as “someone who, due to their personal circumstances, is especially susceptible to harm – particularly when a firm is not acting with appropriate levels of care.” [1]. This view sees vulnerability as a spectrum: anyone can become vulnerable if their circumstances change, with **certain characteristics of vulnerability** (e.g. poor health or cognitive impairment, life events like bereavement or job loss, low emotional resilience, or low capability such as poor literacy/numeracy) increasing that risk.

In the EU, the General Data Protection Regulation (GDPR) does not explicitly define “vulnerable persons” in an article, but its recitals recognise that **children merit specific protection** and are considered “vulnerable natural persons” under data protection law [2]. GDPR guidance and the ICO also note that other individuals — such as the elderly, people with disabilities or mental health conditions, asylum seekers, and the seriously ill — may be vulnerable, often because they “may not have capacity to act or advocate for themselves” (for example, to give fully informed consent) [7]. This means extra care is required when processing their data or communicating with them. 

The proposed EU AI Act takes vulnerability into account as well: it outright prohibits AI systems that exploit the vulnerabilities of specific groups (for instance, based on age, disability, or social/economic disadvantage) in ways that cause or are likely to cause harm. This prohibition covers manipulative or deceptive AI behavior that "exploits other individual vulnerabilities _and_ influences their behaviour or choices to a point of creating significant harm" [8]. In practice, an example given is highly persuasive content targeting people in dire financial situations and causing them financial harm, which would be disallowed. The AI Act and related digital regulations also broaden the scope of who might be considered vulnerable — beyond the traditional categories of minors, the elderly or disabled, new proposals include groups like migrants, people living in poverty, and those receiving public assistance as potentially vulnerable users of AI [9].

In the United States, the Federal Trade Commission (FTC) similarly emphasises protecting vulnerable populations under its mandate to prevent unfair or deceptive practices. The FTC has the authority to regulate marketing practices directed at vulnerable groups (e.g. children or seniors), recognising that these populations can be disproportionately harmed by deception [10]. For example, the FTC reports annually on fraud targeting older adults and has established initiatives (like the Stop Senior Scams Act) to coordinate protection for seniors [11].

Across these regulatory perspectives, a common theme is that vulnerability is understood as an elevated risk of harm or disadvantage due to personal factors (like age, health, or literacy) or situational factors (like isolation or economic hardship). **Organisations deploying AI, especially in customer-facing chat interactions, are expected to be mindful of these factors to avoid exploiting or worsening a user’s vulnerable position.**

### Academic Perspectives
Academically, the concept of vulnerability has been expanded in light of digital technology and AI. Traditionally, vulnerability was often seen as an intrinsic status of particular groups (the old, the young, the infirm, etc.), but recent scholarship introduces the idea of digital vulnerability as a more universal and contextual condition. Rather than being tied solely to inherent traits, digital vulnerability is described as a “universal state of powerlessness present within the digital environment”. In other words, modern online platforms and AI-driven services can create situations where any user – not just those from traditionally vulnerable groups – might experience reduced autonomy or increased risk. Researchers Sax and Helberger (2024) identify three key elements characterizing digital vulnerability: (1) its relational nature (vulnerabilities arise from the relationship between the user and powerful digital systems), (2) its architectural nature (vulnerability can result from the design of technology and interfaces), and (3) the erosion of privacy in data-driven environments. Because of heavy data collection and algorithmic personalization, users face asymmetrical power relationships with technology providers. These power imbalances can produce new vulnerabilities: for instance, a social media platform or AI chatbot can be deliberately engineered to infer or even create vulnerabilities by adjusting what a user sees and how options are presented. A user might be nudged, without their full awareness, into emotional states or choices that benefit the platform at the user’s expense. This academic viewpoint implies that vulnerability is not solely an attribute of the user, but also a result of the system’s behavior and the context of interaction. In AI chat interactions, this means even a digitally savvy person could become vulnerable if the AI manipulates information in a persuasive way or if the conversation leads the user into revealing sensitive information. Scholars argue for contextual and dynamic definitions of vulnerability: a normally capable individual could be rendered vulnerable by certain AI system practices (e.g., persuasive emotional manipulation or exploitation of cognitive biases). Thus, “AI-specific” vulnerabilities include factors like over-reliance on AI outputs, diminished critical thinking in the face of a seemingly authoritative AI, or susceptibility to tailored manipulative content. This broadened understanding complements regulatory approaches: for example, the EU’s digital framework acknowledges both the classic vulnerabilities (specific groups like minors) and these emerging vulnerabilities affecting consumers at large. In summary, academic perspectives encourage viewing vulnerability in AI chats as a fluid condition influenced by personal traits, the design of AI systems, and the surrounding digital context. Effective classification of vulnerability must therefore account for both who the user is and how the AI interaction might be impacting them.

Justification for Classifier Design

Designing a classifier to detect vulnerable users in AI chat requires selecting the right indicators – observable signals in the user’s messages or behavior that strongly suggest vulnerability. We focus on several key indicators: emotional distress, cognitive confusion or impairment, low digital literacy, and related contextual clues (the “etc.” can include social isolation, extreme risk-taking, or other signs of diminished capacity). These factors are justified both by regulatory guidance and real-world evidence as strongly correlated with user vulnerability:

Emotional Distress: Emotional distress encompasses signs of anxiety, depression, panic, anger, or hopelessness in a user’s text. Users who are emotionally distraught may have impaired judgment or be susceptible to harm, including self-harm or exploitation. For example, GDPR-related guidance explicitly lists “people with mental disorders or mental health conditions” as potentially vulnerable persons. A user expressing sadness, desperation, or fear in a chat might be in a mental state where they cannot fully safeguard their own interests. From a duty-of-care perspective, such a user should be identified and possibly given special assistance (or at least not taken advantage of). Emotionally charged language (e.g., “I feel like I can’t go on” or “I’m so scared and I don’t know what to do”) can act as a red flag that the person is in crisis or high stress, indicating vulnerability. Additionally, emotional distress can make individuals more impulsive or less critical – a state that malicious actors exploit by inducing fear or excitement to scam people. Indeed, scammers “exploit…fear [and] trust” to manipulate targets, showing how an emotional state (fearful or overly trusting) is a vulnerability in itself. Therefore, an AI classifier that notices strong negative emotions or mood swings in a user’s messages can infer a higher likelihood that the user is vulnerable or in need of care.

Cognitive Confusion or Impairment: This indicator covers situations where the user appears disoriented, forgetful, or unable to comprehend information that a typical user would. It might manifest as the user repeatedly asking for clarification, contradicting themselves, or using language that suggests a cognitive difficulty (e.g., mentioning memory issues or appearing very confused about basic concepts). Cognitive impairment – whether due to age (like early dementia), intellectual disabilities, or temporary conditions (like being in shock) – is a classic vulnerability factor identified by regulators. The FCA, for instance, notes that “poor health, such as cognitive impairment,” contributes to customer vulnerability. A confused user might not understand the implications of advice given by a chatbot or could misinterpret information, putting them at risk. In AI chat, if someone is struggling to follow along or seems mentally disorganized, the classifier should flag this. Recognizing cognitive confusion is important because these users may need simpler explanations or may be easily misled. In the worst cases, an unscrupulous system (or third-party via the AI) could take advantage of their confusion. Thus, including cognitive clarity as an indicator helps the classifier detect users who might require extra confirmation of understanding or a hand-off to a human assistant.

Low Digital Literacy: Digital literacy refers to a person’s ability to understand and use digital tools and online information safely. A user with low digital literacy might not grasp common online practices, may be unaware of digital risks, or might reveal that they don’t know how to do certain basic tasks (like managing privacy settings, verifying identities, etc.). Such users are particularly vulnerable online – they can fall prey to misinformation, scams, or manipulative interfaces because they lack the knowledge to spot red flags. Evidence of this vulnerability is seen in the prevalence of online fraud among certain groups: “vulnerable populations, especially the elderly and disabled, are increasingly targeted by online scams” and these groups “often lack the digital literacy necessary to recognize the warning signs of a scam.”. In practice, a user who, for example, shares sensitive information naively or asks questions that reveal misconceptions about how the internet or AI works (e.g. “Is this human or a computer?”, “Can I give you my bank PIN to fix the issue?”) would trigger the classifier. Scammers and malicious AI content exploit lack of digital savvy; as one report noted, criminals leverage the very same vulnerabilities – “fear, trust, or a lack of digital literacy” – to influence and deceive people. Therefore, the classifier looks for cues like technical confusion, naive trust in the AI’s identity, or failure to follow basic security practices. Flagging low digital literacy allows the system to adapt its responses (perhaps provide more guidance or warnings) or involve human support to prevent the user from coming to harm.

Other Contextual Indicators (Isolation, Language, etc.): Beyond the three primary categories above, other contextual clues can inform vulnerability. For instance, social isolation or loneliness can be inferred if a user mentions having no one to help them or seems to treat the chatbot as their only confidant. Such isolation can exacerbate vulnerability because the user may be desperate for help or companionship, lowering their guard. In a real-world example, an elderly disabled man’s “social isolation…made him an easy target” for an online romance scam that preyed on his “longing for connection.”. If a user says things like “you’re the only one I can talk to” or “I haven’t told anyone else this,” the AI should register potential vulnerability. Another factor could be extreme financial or personal desperation (e.g., “I’ll do anything to get money for my rent” could indicate the user is economically vulnerable and at risk of exploitation). Even the tone and language complexity a user employs might be relevant – extremely simplistic language or incoherent sentences could imply either a young user, low education level, or cognitive issues, all of which link to vulnerability. It’s important to note that no single indicator is a perfect determinant; instead, the classifier will look at the combination and severity of these signals. For example, mild frustration alone might not mean vulnerability, but frustration coupled with confusion and self-deprecating remarks might. The rationale for designing the classifier around these indicators is to capture a wide range of vulnerable states: emotional, cognitive, and practical/educational. These categories are frequently cited in guidelines on treating users fairly. By detecting them, the AI can either adjust its behavior (providing clearer info, empathy, or refraining from certain actions) or flag the interaction for human review. In summary, each indicator corresponds to a dimension of user vulnerability that is well-documented – emotional distress aligns with mental health risks, cognitive confusion with mental capacity issues, low digital literacy with susceptibility to digital manipulation, and social isolation/trust with exploitable loneliness. Focusing on these signals helps ensure the classifier can catch both overt and subtle signs that a user needs special care, thereby aligning the AI’s behavior with ethical best practices (such as avoiding harm and providing extra support to those who need it most).

Flexible Sensitivity Schema

When implementing a vulnerability classifier, a one-size-fits-all sensitivity can be problematic. In some contexts (e.g. a mental health support chatbot), we may want the system to be extremely sensitive to any hint of distress; in others (e.g. a general customer service bot), an overly sensitive classifier might flag too many false positives and interrupt the experience unnecessarily. To address this, we propose a flexible sensitivity parameter schema that allows tuning the strictness of the classifier’s judgments. This schema could be set by the developers or even adjusted in real-time based on context or user preferences. We outline three levels of sensitivity – Low, Medium, High – along with how the classifier should behave in each mode:

Low Sensitivity: This is a conservative setting where the classifier only flags a user as vulnerable when there are clear, strong indicators present. It prioritizes avoiding false positives. In the low setting, the AI will require multiple or very explicit signals of vulnerability before classifying the user as such. For example, it might act only if a user explicitly says something like “I am feeling suicidal” or if several indicators (e.g. crying emoji + statements of confusion + mention of mental illness) are all detected together. Minor hints of frustration or slight sadness would not trigger a vulnerability label under low sensitivity. This setting might be appropriate for scenarios where interventions are costly or intrusive – for instance, a busy customer support center that only wants to escalate genuinely critical cases. Technically, this corresponds to a high threshold for the classifier: the model might need, say, >90% confidence of vulnerability before flagging. The trade-off is that some genuinely vulnerable users with subtle signals might be missed (higher false negatives), but those flagged are very likely to truly need help.

Medium Sensitivity: The medium setting offers a balanced approach between caution and proactiveness. The classifier will flag vulnerability when there are noticeable indicators, even if they are not extreme, but it will ignore very ambiguous or borderline cases. This might mean the threshold is moderate – catching most cases where a user shows a reasonable level of distress or confusion, while still filtering out noise. For instance, a user repeatedly asking for help and showing signs of agitation would be marked as vulnerable in medium mode, even if they haven’t explicitly said they are in crisis. However, a single mild statement like “I’m a bit upset today” might not immediately trigger it without additional context. Medium sensitivity aims for a good precision-recall balance: it strives to catch vulnerable users with fair reliability while minimizing false alarms. This level could be the default for many applications, providing a safety net for users in need without overwhelming the system (or the users) with constant interventions. In practice, medium sensitivity might align with, say, a 70-80% confidence threshold in the classification model, or using a scoring system where moderate evidence is sufficient for a flag.

High Sensitivity: This is a maximally cautious setting where the classifier errs on the side of identifying vulnerability even on tenuous evidence. In high sensitivity mode, the slightest indicator or hint could result in a user being flagged as potentially vulnerable. The system is tuned to have very few false negatives (i.e., try not to miss anyone who might be vulnerable), accepting that there will be more false positives. For example, if a user’s message includes any emotional language (even something like “I’m having a rough day”), the classifier might flag it and perhaps ask a gentle probing question or switch to a more empathetic tone. High sensitivity might be used in sensitive domains like mental health support, suicide prevention chats, or when interacting with known vulnerable groups (e.g., an AI tutor for children might run in high sensitivity to catch any sign of bullying or distress). The threshold for flagging is low – the model might act on a low confidence (like >50% likelihood of vulnerability). This setting ensures that virtually all potential cases of vulnerability are caught (maximizing recall), which is crucial when missing a case could mean serious harm. The downside is the classifier will likely produce some false alarms, flagging users who are actually fine. To mitigate disruption, the system’s response to a flag at high sensitivity can be mild (for example, just increasing politeness or offering help, rather than calling emergency services outright). In summary, high sensitivity mode treats “better safe than sorry” as the guiding principle.

Implementing these sensitivity levels could be as simple as an adjustable threshold in the model’s decision function, or more complex like switching between different models or prompt formulations tuned for each level. The key benefit of a flexible schema is customization – it allows the AI provider to tailor the classifier’s strictness to the context of use and risk tolerance. For instance, an enterprise might start with medium sensitivity and then adjust to high if they notice they’re still missing some vulnerable users in testing. Or an AI could dynamically raise sensitivity if it detects conversation topics that are inherently higher-risk (imagine a chatbot noticing the user is talking about health or finances might temporarily go to high sensitivity). This schema supports AI ethics by aligning the system’s behavior with the needs of the situation: low sensitivity avoids patronizing or over-policing users, while high sensitivity ensures help is offered wherever needed. Such tuning also reflects NLP best practices in deployment, where models often need calibration to balance precision and recall for the target scenario. In sum, providing low, medium, and high sensitivity options makes the vulnerability classifier versatile and context-aware, enabling a safer and more considerate AI chat experience for all users.

Python Function for Generating a Prompt

To operationalize the above ideas in an AI system, we need to craft a prompt that instructs a language model to perform vulnerability classification according to our specifications. Below is a Python function that generates a classifier prompt string, given two parameters: a sensitivity level (low, medium, or high) and a boolean thinking flag. The prompt is designed with clear instructions and structure so that a large language model can interpret it effectively. It incorporates considerations from AI ethics (e.g., instructing the model to be fair and not make unsupported assumptions) and NLP best practices (e.g., explicitly defining the task, the factors to consider, and the required output format). The output format is XML-based: the model’s answer should be enclosed in specific XML tags, which makes it easy for downstream systems to parse the result. If thinking=True, the model is prompted to include an <analysis> section with its reasoning (this can be used as “chain-of-thought” tokens that might be captured or logged for transparency), followed by the final classification in a <result> tag. If thinking=False, the model should directly output only the <result> tag with the classification, providing no intermediate analysis. This design ensures that we can toggle the verbosity of the model’s answer for debugging or interpretability without altering the core classification logic.

def generate_vulnerability_prompt(sensitivity: str = "medium", thinking: bool = False) -> str:
    """
    Generate a prompt for an AI vulnerability classifier with specified sensitivity and thinking mode.
    
    :param sensitivity: Sensitivity level of the classifier ("low", "medium", or "high").
    :param thinking: If True, instructs the AI to include an <analysis> section with reasoning before the result.
    :return: A formatted prompt string for the classifier.
    """
    # Validate sensitivity input
    level = sensitivity.strip().lower()
    if level not in {"low", "medium", "high"}:
        raise ValueError("sensitivity must be 'low', 'medium', or 'high'")
    
    # Define sensitivity-specific guidance
    if level == "low":
        sensitivity_note = ("Flag the user as vulnerable only if there are strong, clear signs. "
                             "Avoid false positives; only obvious vulnerability indicators should trigger.")
    elif level == "high":
        sensitivity_note = ("Be very alert to any possible sign of vulnerability. "
                             "Even subtle or minor cues should trigger a vulnerability flag to avoid missing anyone in need.")
    else:
        # medium or default
        sensitivity_note = ("Balance sensitivity between false negatives and false positives. "
                             "Flag vulnerability when there are reasonable indicators, but do not overreact to very subtle cues.")
    
    # Define whether to include analysis step
    if thinking:
        analysis_instruction = ("First, provide a brief reasoning in an <analysis> tag explaining relevant signs or context. ")
    else:
        analysis_instruction = ("Skip any detailed reasoning and go straight to the final assessment. ")
    
    analysis_instruction += "Then output the classification in a <result> tag."
    
    # Construct the prompt
    prompt = (
        "You are a classifier that determines if a user in a chat conversation is vulnerable. "
        "Vulnerability means the user may be especially susceptible to harm or unable to fully protect their own interests. \n"
        "Consider the user's messages and look for indicators of vulnerability such as:\n"
        "- Emotional distress (e.g., sadness, anxiety, expressions of fear or hopelessness)\n"
        "- Cognitive confusion or impairment (e.g., disorientation, memory issues, trouble understanding basic info)\n"
        "- Low digital literacy or naivety (e.g., confusion about technology, unsafe sharing of personal info)\n"
        "- Social isolation or other personal hardships mentioned (e.g., the user is very alone or desperate)\n\n"
        f"Sensitivity level: **{level.capitalize()}**. {sensitivity_note}\n\n"
        "Your task: Determine if the user should be considered vulnerable at this sensitivity setting. "
        "Provide the output in an XML format as specified below:\n"
        "<analysis> ... </analysis>\n"
        "<result> ... </result>\n\n"
        f"{analysis_instruction} The <result> should be 'vulnerable' if the user is likely vulnerable, or 'not_vulnerable' otherwise. "
        "Do not include any text outside the XML tags."
    )
    return prompt

# Example usage:
prompt_text = generate_vulnerability_prompt(sensitivity="high", thinking=True)
print(prompt_text)

In this function, we first normalize and validate the sensitivity input. We then prepare a sensitivity_note string that will be inserted into the prompt to modify the classifier’s behavior according to the chosen level. For example, in low sensitivity, the note emphasizes only flagging on strong evidence, whereas in high sensitivity it tells the model to be very alert even to subtle cues. Next, we prepare an analysis_instruction string based on the thinking flag. If thinking is True, we tell the model to produce a reasoning step in an <analysis> tag before giving the final result; if False, we explicitly tell it to skip detailed reasoning and go straight to the classification. This dual approach ensures that the prompt can support an internal “thinking out loud” mode (useful for developers or for transparency) without confusing the end-user output.

The prompt itself (stored in the prompt variable) is a multi-line string that clearly defines the classifier’s role and instructions. We start by framing the AI as a classifier and defining what vulnerability means in this context (a user “susceptible to harm or unable to protect their interests”). We then explicitly list the key indicators it should consider – emotional distress, cognitive confusion, low digital literacy, isolation, etc. – as bullet points in the prompt. Enumerating these factors helps the model focus on relevant aspects of the user’s messages (this reflects NLP best practices of providing examples or criteria in the prompt to guide the model’s attention). We also inject the sensitivity level in bold and include the earlier prepared note about how strictly to interpret the signals at that level. This way, the model’s “mindset” is adjusted: e.g., if sensitivity is high, it knows even a small hint might warrant a flag.

Finally, we describe the required output format: an XML snippet with <analysis> and <result> tags. By showing an example structure in the prompt and explicitly instructing what to put in each tag, we reduce the chance of the model deviating from the format. The <result> tag is specified to contain either “vulnerable” or “not_vulnerable”, which standardizes the classification output. (In a more complex classifier, this could perhaps be a category or risk level, but for simplicity we use a binary decision here.) We also caution the model not to include anything outside the XML, to prevent it from adding extra commentary. This ensures the output is machine-readable and easy to parse by whatever system is using this classifier.

In the example usage at the bottom of the code, we demonstrate generating a prompt for a high sensitivity classifier with thinking=True. The printed prompt_text would look like a well-structured set of instructions to the AI, for example (excerpted for brevity):

You are a classifier that determines if a user in a chat conversation is vulnerable... 
Consider the user's messages and look for indicators of vulnerability such as:
- Emotional distress (e.g., sadness, anxiety, expressions of fear or hopelessness)
- Cognitive confusion or impairment ...
- Low digital literacy or naivety ...
- Social isolation or other personal hardships ...

Sensitivity level: **High**. Be very alert to any possible sign of vulnerability. Even subtle or minor cues should trigger a vulnerability flag to avoid missing anyone in need.

Your task: Determine if the user should be considered vulnerable at this sensitivity setting. Provide the output in an XML format as specified below:
<analysis> ... </analysis>
<result> ... </result>

First, provide a brief reasoning in an <analysis> tag explaining relevant signs or context. Then output the classification in a <result> tag. The <result> should be 'vulnerable' if the user is likely vulnerable, or 'not_vulnerable' otherwise. Do not include any text outside the XML tags.

This prompt is structured to elicit a high-quality interpretation from the AI model. It is thorough in describing what to look for and how to respond, but also mindful of efficiency – it uses clear, concise language and formatting (like bullet points and tag examples) to communicate requirements without unnecessary verbosity. By integrating ethical considerations (e.g., instructing the model on what not to do, like avoid extraneous output or unsupported assumptions) and grounding the instructions in well-defined criteria, we increase the likelihood that the AI will behave predictably and responsibly. The resulting classifier, powered by this prompt, will be in line with the earlier discussed definitions and justifications: it will check for the right vulnerability signals, apply the desired sensitivity threshold, and produce a consistent, parseable output that can be used to trigger appropriate support for the user.

Overall, this report has defined what constitutes user vulnerability in AI chats, explained why we focus on certain tell-tale signs, and provided a practical approach to implement a classifier that is both adaptable and aligned with ethical AI practices. By classifying vulnerable users accurately, AI systems can better protect and serve those users – for example, by avoiding harmful content, offering help, or escalating to human intervention – thereby fulfilling the moral and regulatory expectation that technology should not exploit or neglect individuals who are at greater risk.

## Sources

[^1]: Financial Conduct Authority – Guidance on the Fair Treatment of Vulnerable Customers

[^2]: GDPR Recital 75 – Vulnerable Data Subjects (Children specifically); GDPR and Vulnerable People (ICO)

[^3]: EU Artificial Intelligence Act – Prohibited AI Practices (Exploitation of Vulnerabilities); Academic analysis of digital vulnerability in EU law

[^4]: Pomeranz, FTC Authority to Regulate Marketing to Children (and vulnerable populations); FTC Press Release on Protecting Older Adults (2024)

[^5]: Maastricht University. (2023, February). Digital vulnerability of the AI-assisted consumers. Retrieved from https://www.maastrichtuniversity.nl/blog/2023/02/digital-vulnerability-ai-assisted-consumers

[^6]: New America, Empowering Vulnerable Communities Against Online Scams – example of scams exploiting low digital literacy and trust

[^7]: Vulnerability Registration Service. (n.d.). Data, vulnerability & GDPR: Considerations for businesses. Retrieved March 5, 2025, from https://www.vulnerabilityregistrationservice.co.uk/data-protection-gdpr-and-vulnerability/

[^8]: Shrishak, K. (2025, February 21). EU's AI Act: Tread the guidelines lightly. TechPolicy.Press. https://www.techpolicy.press/eu-ai-act-tread-the-guidelines-lightly/

[^9]: Sax, M., & Helberger, N. (2024). Digital vulnerability and manipulation in the emerging digital framework. In N. Helberger, B. Kas, H.-W. Micklitz, M. Namysłowska, L. Naudts, P. Rott, M. Sax, & M. Veale (Eds.), Digital fairness for consumers (pp. 10–24). BEUC. https://pure.uva.nl/ws/files/181992648/Digital_vulnerability_BEUC-X-2024-032.pdf

[^10]: Pomeranz, J. L. (2011). Federal Trade Commission's authority to regulate marketing to children: Deceptive vs. unfair rulemaking. Health Matrix: Journal of Law-Medicine, 21(2), 521–553.

[^11]: Federal Trade Commission. (2024, October 30). FTC issues annual report to Congress on agency's actions to protect older adults. Retrieved from https://www.ftc.gov/news-events/news/press-releases/2024/10/ftc-issues-annual-report-congress-agencys-actions-protect-older-adults