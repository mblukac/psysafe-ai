from typing import Optional, Dict, Any
import aisuite as ai

from utils.llm_utils import get_llm_response
from prompts.vulnerability.vulnerability import build_vulnerability_prompt
from prompts.vulnerability.vulnerability import Sensitivity, VulnerabilityIndicators
import re


def analyze_text_vulnerability(
    text: str,
    model: str = "anthropic:claude-3-5-sonnet-20240620",
    indicators: str = "both",
    sensitivity: str = "medium",
    reasoning: bool = True,
    confidence: bool = True,
    client: Optional[ai.Client] = None
) -> Dict[str, Any]:
    """
    Analyze text for vulnerability indicators using an LLM.
    This is a wrapper around the vulnerability detection module.
    
    Args:
        text: The text to analyze
        model: The model identifier in the format "provider:model_name"
        indicators: Which indicators to consider ('external', 'internal', or 'both')
        sensitivity: Detection sensitivity level ('low', 'medium', or 'high')
        reasoning: Whether to include reasoning in the response
        confidence: Whether to include a confidence score in the response
        client: Optional pre-configured client. If None, a new client will be created.
    
    Returns:
        Dictionary with vulnerability analysis results
    """
    
    # Convert string parameters to appropriate enums
    sensitivity_enum = Sensitivity[sensitivity.upper()]
    indicators_enum = VulnerabilityIndicators[indicators.upper()]
    
    # Build the vulnerability analysis prompt
    prompt = build_vulnerability_prompt(
        user_context=text,
        indicators=indicators_enum,
        sensitivity=sensitivity_enum,
        reasoning=reasoning,
        confidence=confidence
    )
    
    # Call the LLM with the vulnerability prompt
    response = get_llm_response(
        model=model,
        prompt=prompt,
        client=client
    )
    
    # Parse the XML response
    result = {}
    
    try:
        # Quick and dirty XML parsing - you might want to replace this with proper XML handling
        if reasoning:
            analysis_match = re.search(r"<analysis>(.*?)</analysis>", response, re.DOTALL)
            result["analysis"] = analysis_match.group(1).strip() if analysis_match else None
        
        vulnerable_match = re.search(r"<vulnerable>(.*?)</vulnerable>", response, re.DOTALL)
        result["vulnerable"] = vulnerable_match.group(1).strip().lower() == "true" if vulnerable_match else None
        
        if confidence:
            confidence_match = re.search(r"<confidence>(.*?)</confidence>", response, re.DOTALL)
            result["confidence"] = float(confidence_match.group(1).strip()) if confidence_match else None
        
        reason_match = re.search(r"<reason>(.*?)</reason>", response, re.DOTALL)
        result["reason"] = reason_match.group(1).strip() if reason_match else None
    
    except Exception as e:
        result["error"] = f"Failed to parse response: {str(e)}"
        result["raw_response"] = response
    
    return result

