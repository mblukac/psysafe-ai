from typing import List, Dict, Any, Optional
import logging

from psysafe.catalog.base.llm_guardrail import LLMGuardrail
from psysafe.core.config import VulnerabilityConfig
from psysafe.core.types import (
    VulnerabilityCheckOutput, 
    GuardrailResponse, 
    SensitivityLevel,
    VulnerabilityIndicators
)
from psysafe.core.models import Conversation, GuardedRequest
from psysafe.core.exceptions import (
    LLMDriverError,
    ResponseParsingError,
    GuardrailError
)
from psysafe.utils.parsing import ResponseParser
from psysafe.core.template import PromptTemplate, PromptRenderCtx
from psysafe.typing.requests import OpenAIChatRequest
from psysafe.catalog import GuardrailCatalog


class VulnerabilityDetectionGuardrail(LLMGuardrail[VulnerabilityConfig]):
    """
    A guardrail to detect signs of vulnerability in user communication.
    """

    def __init__(
        self,
        config: Optional[VulnerabilityConfig] = None,
        driver: Optional[Any] = None,
        **kwargs
    ):
        """
        Initializes the VulnerabilityDetectionGuardrail.

        Args:
            config: VulnerabilityConfig instance or None to use defaults
            driver: Optional LLM driver instance
            **kwargs: Additional config parameters for backward compatibility
        """
        # Handle backward compatibility
        if config is None:
            # Extract old-style parameters
            indicators = kwargs.get('indicators', list(VulnerabilityIndicators))
            sensitivity = kwargs.get('sensitivity', SensitivityLevel.MEDIUM)
            reasoning = kwargs.get('reasoning', True)
            confidence = kwargs.get('confidence', False)
            
            # Convert old Sensitivity enum to new SensitivityLevel if needed
            if hasattr(sensitivity, 'value') and sensitivity.value in ['low', 'medium', 'high']:
                sensitivity = SensitivityLevel(sensitivity.value)
            
            config = VulnerabilityConfig(
                indicators=indicators,
                sensitivity=sensitivity,
                reasoning_enabled=reasoning,
                confidence_enabled=confidence
            )
        
        super().__init__(config, driver)
        
        # Load template
        self.template = PromptTemplate.from_file(
            "psysafe/catalog/vulnerability_detection/prompt.md"
        )
        self.logger = logging.getLogger(__name__)
        self.parser = ResponseParser(self.logger)

    def _get_vulnerability_indicators_text(self) -> str:
        """
        Constructs the text block for vulnerability indicators.
        """
        indicator_map = {
            VulnerabilityIndicators.HEALTH_CONDITIONS: (
                "- Health conditions: This includes physical disabilities, severe or long-term illnesses, "
                "hearing or visual impairments, and mental health conditions such as depression, anxiety, "
                "or cognitive impairment."
            ),
            VulnerabilityIndicators.LIFE_EVENTS: (
                "- Recent life events: This includes events such as bereavement, job loss, relationship breakdown, "
                "or other significant personal changes that might impact stability and emotional well-being."
            ),
            VulnerabilityIndicators.RESILIENCE: (
                "- Low resilience: This refers to an individual's ability to cope with or recover from "
                "stressful situations. Low resilience might manifest as an inability to handle pressure, "
                "poor problem-solving skills, or a lack of a support network."
            ),
            VulnerabilityIndicators.CAPABILITY: (
                "- Capability issues: This includes low knowledge or confidence in managing finances, poor literacy "
                "or numeracy skills, communication difficulties (e.g., understanding complex information), "
                "or lack of digital skills."
            ),
        }
        return "\n".join(
            [indicator_map[indicator] for indicator in self.config.indicators]
        )

    def _get_vulnerability_sensitivity_text(self) -> str:
        """
        Constructs the text block for vulnerability sensitivity.
        """
        if self.config.sensitivity == SensitivityLevel.LOW:
            return (
                "- You will classify the user as vulnerable only if there are clear and explicit statements "
                "indicating vulnerability. Ambiguous or indirect references should not be sufficient for "
                "a 'vulnerable' classification."
            )
        elif self.config.sensitivity == SensitivityLevel.MEDIUM:
            return (
                "- You will classify the user as vulnerable if there are explicit statements or strong implicit "
                "indicators of vulnerability. You should consider the context and look for patterns that suggest "
                "vulnerability, even if not overtly stated."
            )
        elif self.config.sensitivity == SensitivityLevel.HIGH:
            return (
                "- You will classify the user as vulnerable if there are any explicit or implicit indicators, "
                "even if subtle. You should err on the side of caution and flag potential vulnerability if there "
                "is any doubt. Prioritize safety and potential risk over precision."
            )
        return ""  # Should not happen

    def _generate_prompt(self, conversation: Conversation) -> str:
        """Generate prompt for LLM based on conversation
        
        Args:
            conversation: Input conversation
            
        Returns:
            Formatted prompt string
        """
        # Extract user messages content
        user_messages_content: List[str] = []
        for msg in conversation.messages:
            if msg.role == "user" and msg.content:
                user_messages_content.append(msg.content)
        
        user_context = "\n".join(user_messages_content)

        vulnerability_indicators_text = self._get_vulnerability_indicators_text()
        vulnerability_sensitivity_text = self._get_vulnerability_sensitivity_text()

        render_ctx = PromptRenderCtx(
            driver_type="openai",  # Default, could be made configurable
            model_name="unknown",
            request_type="chat",
            variables={
                "user_context": user_context,
                "vulnerability_indicators_text": vulnerability_indicators_text,
                "vulnerability_sensitivity_text": vulnerability_sensitivity_text,
                "reasoning": self.config.reasoning_enabled,
                "confidence": self.config.confidence_enabled,
            }
        )

        return self.template.render(render_ctx)

    def _call_llm(self, prompt: str) -> str:
        """Call LLM with prompt and return raw response
        
        Args:
            prompt: Formatted prompt
            
        Returns:
            Raw LLM response
            
        Raises:
            LLMDriverError: If LLM call fails
        """
        if not hasattr(self.driver, 'send'):
            raise LLMDriverError(
                f"Driver {type(self.driver).__name__} does not support 'send' method",
                guardrail_name=self.__class__.__name__
            )
        
        # Construct request for LLM
        llm_request = {
            "messages": [{"role": "system", "content": prompt}],
            "temperature": self.config.temperature,
            "max_tokens": self.config.max_tokens
        }
        
        try:
            response = self.driver.send(llm_request)
            
            # Extract content from response
            if response and response.get("choices"):
                first_choice = response["choices"][0]
                if first_choice and first_choice.get("message"):
                    content = first_choice["message"].get("content")
                    if content:
                        return content
            
            raise LLMDriverError(
                "Could not extract content from LLM response",
                guardrail_name=self.__class__.__name__,
                raw_response=str(response)
            )
            
        except Exception as e:
            if isinstance(e, LLMDriverError):
                raise
            raise LLMDriverError(
                f"LLM call failed: {str(e)}",
                guardrail_name=self.__class__.__name__
            )

    def check(self, conversation: Conversation) -> GuardrailResponse:
        """Check conversation using LLM
        
        Args:
            conversation: Input conversation
            
        Returns:
            GuardrailResponse with check results
        """
        try:
            # Ensure driver is available
            self._ensure_driver()
            
            # Generate prompt
            prompt = self._generate_prompt(conversation)
            
            # Call LLM
            raw_response = self._call_llm(prompt)
            
            # Parse response to VulnerabilityCheckOutput
            vulnerability_output = self.parser.parse_to_model(raw_response, VulnerabilityCheckOutput)
            
            # Convert VulnerabilityCheckOutput to GuardrailResponse
            response = GuardrailResponse(
                is_triggered=vulnerability_output.is_vulnerable,
                risk_score=vulnerability_output.confidence_score,
                details={
                    "is_vulnerable": vulnerability_output.is_vulnerable,
                    "confidence_score": vulnerability_output.confidence_score,
                    "severity_level": vulnerability_output.severity_level.value if vulnerability_output.severity_level else None,
                    "indicators_detected": [ind.value for ind in vulnerability_output.indicators_detected],
                    "reasoning": vulnerability_output.reasoning,
                },
                raw_llm_response=raw_response,
                metadata=vulnerability_output.metadata
            )
            
            return response
            
        except ResponseParsingError as e:
            self.logger.error(f"Failed to parse LLM response: {e}")
            return GuardrailResponse(
                is_triggered=False,
                errors=[f"Failed to parse LLM response: {str(e)}"],
                raw_llm_response=e.raw_response,
                metadata={"error_type": "ResponseParsingError"}
            )
        except LLMDriverError as e:
            self.logger.error(f"LLM driver error: {e}")
            return GuardrailResponse(
                is_triggered=False,
                errors=[f"LLM driver error: {str(e)}"],
                metadata={"error_type": "LLMDriverError"}
            )
        except Exception as e:
            self.logger.error(f"Unexpected error in check: {e}")
            return GuardrailResponse(
                is_triggered=False,
                errors=[f"Unexpected error: {str(e)}"],
                metadata={"error_type": type(e).__name__}
            )

    def apply(self, request: OpenAIChatRequest) -> GuardedRequest[OpenAIChatRequest]:
        """
        Applies the guardrail to an incoming request.

        Args:
            request: The incoming OpenAIChatRequest.

        Returns:
            A GuardedRequest object containing the original and modified request.
        """
        user_messages_content: List[str] = []
        messages = request.get("messages", [])
        for msg in messages:
            if msg.get("role") == "user" and isinstance(msg.get("content"), str):
                user_messages_content.append(msg["content"])
        
        user_context = "\n".join(user_messages_content)

        vulnerability_indicators_text = self._get_vulnerability_indicators_text()
        vulnerability_sensitivity_text = self._get_vulnerability_sensitivity_text()

        render_ctx = PromptRenderCtx(
            driver_type=request.get("driver_type", "openai"),
            model_name=request.get("model", "unknown"),
            request_type="chat",
            variables={
                "user_context": user_context,
                "vulnerability_indicators_text": vulnerability_indicators_text,
                "vulnerability_sensitivity_text": vulnerability_sensitivity_text,
                "reasoning": self.config.reasoning_enabled,
                "confidence": self.config.confidence_enabled,
            }
        )

        rendered_prompt = self.template.render(render_ctx)

        modified_request = request.copy()
        
        # Ensure 'messages' key exists and is a list
        if "messages" not in modified_request or not isinstance(modified_request.get("messages"), list):
            modified_request["messages"] = []

        modified_request["messages"].insert(
            0, {"role": "system", "content": rendered_prompt}
        )
        
        # Create metadata for GuardedRequest
        metadata: Dict[str, Any] = {
            "guardrail_name": "VulnerabilityDetectionGuardrail",
            "applied_prompt": rendered_prompt,
            "indicators_used": ",".join([ind.value for ind in self.config.indicators]),
            "sensitivity_level": self.config.sensitivity.value,
            "reasoning_enabled": self.config.reasoning_enabled,
            "confidence_enabled": self.config.confidence_enabled,
        }

        return GuardedRequest(
            original_request=request,
            modified_request=modified_request,
            is_modified=True,
            metadata=metadata,
        )

    def validate(self, response: Any) -> Any:
        """Validate a response against the guardrail.
        
        For vulnerability detection, we don't perform post-response validation.
        The main logic is in the check method which analyzes conversations.
        
        Args:
            response: The response to validate
            
        Returns:
            ValidationReport indicating the response is valid
        """
        from psysafe.core.models import ValidationReport
        
        # For this guardrail, we don't validate responses
        # The main functionality is in the check method
        return ValidationReport(
            is_valid=True,
            violations=[],
            metadata={"guardrail": "vulnerability_detection"}
        )


# Register the guardrail with the catalog
GuardrailCatalog.register("vulnerability_detection", VulnerabilityDetectionGuardrail)