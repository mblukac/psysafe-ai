from enum import Enum
from typing import List, Dict, Any, Optional
import logging # Added import

from psysafe.core.prompt import PromptGuardrail
from psysafe.core.template import PromptTemplate, PromptRenderCtx
from psysafe.core.models import GuardedRequest, CheckOutput, Conversation, Message # Added CheckOutput, Conversation, Message
from psysafe.typing.requests import OpenAIChatRequest, OpenAIMessage # Assuming OpenAIChatRequest
from psysafe.catalog import GuardrailCatalog
from utils.llm_utils import parse_llm_response, LLMResponseParseError # Added import


class Sensitivity(Enum):
    LOW = "low"
    MEDIUM = "medium"
    HIGH = "high"


class VulnerabilityIndicators(Enum):
    HEALTH_CONDITIONS = "health_conditions"
    LIFE_EVENTS = "life_events"
    RESILIENCE = "resilience"
    CAPABILITY = "capability"


class VulnerabilityDetectionGuardrail(PromptGuardrail[OpenAIChatRequest, Any]): # Added type hints
    """
    A guardrail to detect signs of vulnerability in user communication.
    """

    def __init__(
        self,
        indicators: List[VulnerabilityIndicators],
        sensitivity: Sensitivity,
        reasoning: bool = True,
        confidence: bool = False,
    ):
        """
        Initializes the VulnerabilityDetectionGuardrail.

        Args:
            indicators: A list of vulnerability indicators to focus on.
            sensitivity: The sensitivity level for detection.
            reasoning: Whether to include reasoning in the prompt.
            confidence: Whether to include confidence score in the prompt.
        """
        template = PromptTemplate.from_file(
            "psysafe/catalog/vulnerability_detection/prompt.md"
        )
        super().__init__(template=template)
        self.indicators = indicators
        self.sensitivity = sensitivity
        self.reasoning = reasoning
        self.confidence = confidence
        self.logger = logging.getLogger(__name__) # Added logger

    def _get_vulnerability_indicators_text(self) -> str:
        """
        Constructs the text block for vulnerability indicators.
        """
        indicator_map = {
            VulnerabilityIndicators.HEALTH_CONDITIONS: (
                "- Health conditions: This includes physical disabilities, severe or long-term illnesses, "
                "hearing or visual impairments, and mental health conditions such as depression, anxiety, "
                "or cognitive impairment."
            ),
            VulnerabilityIndicators.LIFE_EVENTS: (
                "- Recent life events: This includes events such as bereavement, job loss, relationship breakdown, "
                "or other significant personal changes that might impact stability and emotional well-being."
            ),
            VulnerabilityIndicators.RESILIENCE: (
                "- Low resilience: This refers to an individualâ€™s ability to cope with or recover from "
                "stressful situations. Low resilience might manifest as an inability to handle pressure, "
                "poor problem-solving skills, or a lack of a support network."
            ),
            VulnerabilityIndicators.CAPABILITY: (
                "- Capability issues: This includes low knowledge or confidence in managing finances, poor literacy "
                "or numeracy skills, communication difficulties (e.g., understanding complex information), "
                "or lack of digital skills."
            ),
        }
        return "\n".join(
            [indicator_map[indicator] for indicator in self.indicators]
        )

    def _get_vulnerability_sensitivity_text(self) -> str:
        """
        Constructs the text block for vulnerability sensitivity.
        """
        if self.sensitivity == Sensitivity.LOW:
            return (
                "- You will classify the user as vulnerable only if there are clear and explicit statements "
                "indicating vulnerability. Ambiguous or indirect references should not be sufficient for "
                "a 'vulnerable' classification."
            )
        elif self.sensitivity == Sensitivity.MEDIUM:
            return (
                "- You will classify the user as vulnerable if there are explicit statements or strong implicit "
                "indicators of vulnerability. You should consider the context and look for patterns that suggest "
                "vulnerability, even if not overtly stated."
            )
        elif self.sensitivity == Sensitivity.HIGH:
            return (
                "- You will classify the user as vulnerable if there are any explicit or implicit indicators, "
                "even if subtle. You should err on the side of caution and flag potential vulnerability if there "
                "is any doubt. Prioritize safety and potential risk over precision."
            )
        return "" # Should not happen

    def apply(self, request: OpenAIChatRequest) -> GuardedRequest[OpenAIChatRequest]:
        """
        Applies the guardrail to an incoming request.

        Args:
            request: The incoming OpenAIChatRequest.

        Returns:
            A GuardedRequest object containing the original and modified request.
        """
        user_messages_content: List[str] = []
        messages = request.get("messages", [])
        for msg in messages:
            if msg.get("role") == "user" and isinstance(msg.get("content"), str):
                user_messages_content.append(msg["content"])
        
        user_context = "\n".join(user_messages_content)

        vulnerability_indicators_text = self._get_vulnerability_indicators_text()
        vulnerability_sensitivity_text = self._get_vulnerability_sensitivity_text()

        render_ctx = PromptRenderCtx(
            driver_type=request.get("driver_type", "openai"),
            model_name=request.get("model", "unknown"),
            request_type="chat",
            variables={
                "user_context": user_context,
                "vulnerability_indicators_text": vulnerability_indicators_text,
                "vulnerability_sensitivity_text": vulnerability_sensitivity_text,
                "reasoning": self.reasoning,
                "confidence": self.confidence,
            }
        )

        rendered_prompt = self.template.render(render_ctx)

        modified_request = request.copy() # Ensure we don't modify the original in place if it's mutable
        
        # Ensure 'messages' key exists and is a list
        if "messages" not in modified_request or not isinstance(modified_request.get("messages"), list):
            modified_request["messages"] = []

        modified_request["messages"].insert(
            0, {"role": "system", "content": rendered_prompt}
        )
        
        # Create metadata for GuardedRequest
        # This is a placeholder; actual metadata might be more complex
        metadata: Dict[str, Any] = {
            "guardrail_name": "VulnerabilityDetectionGuardrail",
            "applied_prompt": rendered_prompt,
            "indicators_used": [ind.value for ind in self.indicators],
            "sensitivity_level": self.sensitivity.value,
            "reasoning_enabled": self.reasoning,
            "confidence_enabled": self.confidence,
        }

        return GuardedRequest(
            original_request=request,
            modified_request=modified_request,
            is_modified=True,
            metadata=metadata,
        )

    def _format_conversation_for_apply(self, conversation: Conversation) -> OpenAIChatRequest:
        """
        Formats a Conversation object into the OpenAIChatRequest structure
        expected by the apply method.
        """
        messages_for_llm: List[Dict[str, str]] = []
        for msg in conversation.messages:
            messages_for_llm.append({"role": msg.role, "content": msg.content})
        
        # The apply method expects a dict-like structure for OpenAIChatRequest
        return {"messages": messages_for_llm} # type: ignore

    def check(self, conversation: Conversation) -> CheckOutput:
        """
        Checks a conversation for vulnerability using the bound LLM driver.

        Args:
            conversation: The Conversation object to check.

        Returns:
            A CheckOutput object with the assessment results.
        """
        if not self.driver:
            raise RuntimeError(
                "LLM driver not bound. Please call `guardrail.bind(driver)` before using `check`."
            )

        llm_request_input = self._format_conversation_for_apply(conversation)
        guarded_request = self.apply(llm_request_input)
        modified_llm_request = guarded_request.modified_request

        raw_llm_response_content: Optional[str] = None
        llm_errors: List[str] = []
        llm_metadata: Dict[str, Any] = {}

        try:
            if hasattr(self.driver, 'send'):
                llm_response_dict = self.driver.send(modified_llm_request) # type: ignore
                
                if llm_response_dict and llm_response_dict.get("choices"):
                    first_choice = llm_response_dict["choices"][0]
                    if first_choice and first_choice.get("message"):
                        raw_llm_response_content = first_choice["message"].get("content")
                
                if not raw_llm_response_content:
                    llm_errors.append("Could not extract content from LLM response.")
                    llm_metadata["raw_llm_response_dict"] = llm_response_dict
                    return CheckOutput(
                        is_triggered=False,
                        errors=llm_errors,
                        raw_llm_response=raw_llm_response_content,
                        metadata=llm_metadata
                    )
            else:
                llm_errors.append(f"Bound driver of type {type(self.driver).__name__} does not have a 'send' method for LLM calls.")
                return CheckOutput(is_triggered=False, errors=llm_errors, metadata={"info": "LLM call not possible with this driver.", **llm_metadata})

        except Exception as e:
            llm_errors.append(f"Error during LLM call: {str(e)}")
            return CheckOutput(is_triggered=False, errors=llm_errors, raw_llm_response=str(e), metadata=llm_metadata)

        if not raw_llm_response_content:
            llm_errors.append("LLM response content was empty or could not be extracted after driver call.")
            return CheckOutput(is_triggered=False, errors=llm_errors, raw_llm_response=raw_llm_response_content, metadata=llm_metadata)

        try:
            llm_output = parse_llm_response(raw_llm_response_content, logger=self.logger)
            llm_metadata["parsed_response_type"] = type(llm_output).__name__

            vulnerability_detected = llm_output.get("vulnerability_detected", False) # Default to False if not present
            severity_score_raw = llm_output.get("severity_score")
            cwe_id = llm_output.get("cwe_id")
            explanation = llm_output.get("explanation")

            final_is_triggered: bool = False
            if isinstance(vulnerability_detected, bool):
                final_is_triggered = vulnerability_detected
            elif isinstance(vulnerability_detected, str):
                 # Handle string "true"/"false" case-insensitively
                final_is_triggered = vulnerability_detected.lower() == "true"
            else:
                llm_errors.append(f"LLM output contained an invalid 'vulnerability_detected' value: {vulnerability_detected}")


            final_severity_score: Optional[float] = None
            if severity_score_raw is not None:
                try:
                    final_severity_score = float(severity_score_raw)
                except (ValueError, TypeError):
                    llm_errors.append(f"LLM output contained a non-numeric 'severity_score' value: {severity_score_raw}")
            
            current_details = {
                "cwe_id": cwe_id,
                "explanation": explanation,
                "reasoning": llm_output.get("reasoning"), # If prompt asks for reasoning
                "confidence": llm_output.get("confidence"), # If prompt asks for confidence
                "parsed_llm_output": llm_output
            }
            # Filter out None values from details for cleaner output
            current_details = {k: v for k, v in current_details.items() if v is not None}
            llm_metadata["constructed_details_preview"] = str(current_details)[:200]

            return CheckOutput(
                is_triggered=final_is_triggered,
                risk_score=final_severity_score, # Using severity_score as risk_score
                details=current_details,
                raw_llm_response=raw_llm_response_content,
                errors=llm_errors,
                metadata=llm_metadata
            )
        except LLMResponseParseError as e:
            self.logger.error(f"LLMResponseParseError in check method: {e.message}, Raw Response: {e.raw_response[:200]}")
            llm_errors.append(f"Failed to parse LLM response: {e.message}. Response snippet: {e.raw_response[:500] if e.raw_response else 'N/A'}")
            return CheckOutput(
                is_triggered=False,
                errors=llm_errors,
                raw_llm_response=e.raw_response,
                metadata={"warning": "Could not parse LLM output.", "parser_error_type": type(e).__name__, **llm_metadata}
            )
        except Exception as e:
            self.logger.error(f"Unexpected error processing LLM response after parsing attempt: {str(e)}")
            llm_errors.append(f"Unexpected error processing LLM response: {str(e)}. Response snippet: {raw_llm_response_content[:500] if raw_llm_response_content else 'N/A'}")
            return CheckOutput(
                is_triggered=False,
                errors=llm_errors,
                raw_llm_response=raw_llm_response_content,
                metadata={"warning": "Unexpected error during LLM response processing.", "error_type": type(e).__name__, **llm_metadata}
            )

# Register the guardrail with the catalog
GuardrailCatalog.register("vulnerability_detection", VulnerabilityDetectionGuardrail)