# tests/catalog/test_vulnerability_guardrail.py
import pytest
import json # Added import
from unittest.mock import patch, MagicMock

from psysafe.catalog.vulnerability_detection.guardrail import (
    VulnerabilityDetectionGuardrail,
    Sensitivity,
    VulnerabilityIndicators
)
from psysafe.core.template import PromptTemplate
from psysafe.typing.requests import OpenAIChatRequest, OpenAIMessage
from psysafe.core.models import Conversation, Message, CheckOutput # Added imports
from utils.llm_utils import LLMResponseParseError # Added import

@pytest.mark.parametrize("sensitivity", list(Sensitivity))
@pytest.mark.parametrize("indicators", list(VulnerabilityIndicators))
@pytest.mark.parametrize("reasoning", [True, False])
@pytest.mark.parametrize("confidence", [True, False])
def test_vulnerability_detection_guardrail_initialization(
    sensitivity, indicators, reasoning, confidence
):
    """
    Tests initialization of VulnerabilityDetectionGuardrail with all
    valid combinations of Sensitivity, VulnerabilityIndicators, reasoning, and confidence.
    """
    guardrail = VulnerabilityDetectionGuardrail(
        sensitivity=sensitivity,
        indicators=[indicators], # Expects a list
        reasoning=reasoning,
        confidence=confidence
    )
    assert guardrail.sensitivity == sensitivity
    assert guardrail.indicators == [indicators]
    assert guardrail.reasoning == reasoning
    assert guardrail.confidence == confidence
    assert isinstance(guardrail.template, PromptTemplate)
    assert guardrail.logger is not None # Check logger is initialized

def test_vulnerability_detection_guardrail_initialization_default_flags():
    """Tests initialization with default reasoning and confidence flags."""
    guardrail = VulnerabilityDetectionGuardrail(
        sensitivity=Sensitivity.LOW,
        indicators=[VulnerabilityIndicators.HEALTH_CONDITIONS]
    )
    assert guardrail.reasoning is True 
    assert guardrail.confidence is False

def test_vulnerability_detection_guardrail_initialization_multiple_indicators():
    """Tests initialization with multiple vulnerability indicators."""
    indicators = [
        VulnerabilityIndicators.LIFE_EVENTS, 
        VulnerabilityIndicators.HEALTH_CONDITIONS
    ]
    guardrail = VulnerabilityDetectionGuardrail(
        sensitivity=Sensitivity.HIGH,
        indicators=indicators
    )
    assert guardrail.indicators == indicators

@patch("psysafe.catalog.vulnerability_detection.guardrail.PromptTemplate.from_file")
def test_vulnerability_detection_guardrail_apply_method(mock_from_file, mocker):
    """Tests the apply method of VulnerabilityDetectionGuardrail."""
    mock_template_instance = MagicMock(spec=PromptTemplate)
    mock_render_output = "Rendered Vulnerability Prompt"
    mock_template_instance.render.return_value = mock_render_output
    mock_from_file.return_value = mock_template_instance

    guardrail = VulnerabilityDetectionGuardrail(
        sensitivity=Sensitivity.MEDIUM,
        indicators=[VulnerabilityIndicators.LIFE_EVENTS],
        reasoning=True,
        confidence=True
    )

    original_request: OpenAIChatRequest = { # type: ignore
        "messages": [
            {"role": "system", "content": "Initial system message."},
            {"role": "user", "content": "User says something."},
        ],
        "model": "test-model",
        "driver_type": "openai"
    }
    
    modified_request_obj = guardrail.apply(original_request) # Renamed variable
    modified_request = modified_request_obj.modified_request # Access the dict

    mock_from_file.assert_called_once_with(
        "psysafe/catalog/vulnerability_detection/prompt.md"
    )
    
    assert mock_template_instance.render.call_count == 1
    render_call_args = mock_template_instance.render.call_args
    assert render_call_args is not None
    render_ctx = render_call_args[0][0] 

    assert render_ctx.variables["user_context"] == "User says something."
    assert "Recent life events:" in render_ctx.variables["vulnerability_indicators_text"]
    assert "strong implicit indicators" in render_ctx.variables["vulnerability_sensitivity_text"]
    assert render_ctx.variables["reasoning"] is True
    assert render_ctx.variables["confidence"] is True

    assert len(modified_request["messages"]) == 3
    assert modified_request["messages"][0].get("role") == "system"
    assert modified_request["messages"][0].get("content") == mock_render_output
    assert modified_request["messages"][1].get("role") == "system"
    assert modified_request["messages"][1].get("content") == "Initial system message."
    assert modified_request["messages"][2].get("role") == "user"
    assert modified_request["messages"][2].get("content") == "User says something."

@patch("psysafe.catalog.vulnerability_detection.guardrail.PromptTemplate.from_file")
def test_vulnerability_detection_guardrail_apply_no_initial_system_message(mock_from_file, mocker):
    """Tests apply when the original request has no system message."""
    mock_template_instance = MagicMock(spec=PromptTemplate)
    mock_render_output = "Rendered Vulnerability Prompt No System"
    mock_template_instance.render.return_value = mock_render_output
    mock_from_file.return_value = mock_template_instance

    guardrail = VulnerabilityDetectionGuardrail(
        sensitivity=Sensitivity.LOW,
        indicators=[VulnerabilityIndicators.RESILIENCE]
    )

    original_request: OpenAIChatRequest = { # type: ignore
        "messages": [{"role": "user", "content": "Just a user message."}],
        "model": "test-model",
        "driver_type": "openai"
    }
    modified_request_obj = guardrail.apply(original_request) # Renamed variable
    modified_request = modified_request_obj.modified_request # Access the dict

    mock_from_file.assert_called_once_with(
        "psysafe/catalog/vulnerability_detection/prompt.md"
    )
    mock_template_instance.render.assert_called_once()
    render_ctx = mock_template_instance.render.call_args[0][0]
    assert render_ctx.variables["user_context"] == "Just a user message."

    assert len(modified_request["messages"]) == 2
    assert modified_request["messages"][0].get("role") == "system"
    assert modified_request["messages"][0].get("content") == mock_render_output
    assert modified_request["messages"][1].get("role") == "user"
    assert modified_request["messages"][1].get("content") == "Just a user message."

def test_get_vulnerability_indicators_text():
    """Tests the _get_vulnerability_indicators_text helper method."""
    guardrail = VulnerabilityDetectionGuardrail(
        sensitivity=Sensitivity.LOW, 
        indicators=[
            VulnerabilityIndicators.LIFE_EVENTS,
            VulnerabilityIndicators.HEALTH_CONDITIONS
        ]
    )
    text = guardrail._get_vulnerability_indicators_text()
    assert "Recent life events:" in text
    assert "Health conditions:" in text

def test_get_vulnerability_indicators_text_empty():
    """Tests _get_vulnerability_indicators_text with no indicators."""
    guardrail = VulnerabilityDetectionGuardrail(
        sensitivity=Sensitivity.LOW,
        indicators=[] 
    )
    text = guardrail._get_vulnerability_indicators_text()
    assert text == ""

def test_get_vulnerability_sensitivity_text():
    """Tests the _get_vulnerability_sensitivity_text helper method."""
    guardrail_low = VulnerabilityDetectionGuardrail(sensitivity=Sensitivity.LOW, indicators=[])
    text_low = guardrail_low._get_vulnerability_sensitivity_text()
    assert "clear and explicit statements" in text_low

    guardrail_medium = VulnerabilityDetectionGuardrail(sensitivity=Sensitivity.MEDIUM, indicators=[])
    text_medium = guardrail_medium._get_vulnerability_sensitivity_text()
    assert "strong implicit indicators" in text_medium

    guardrail_high = VulnerabilityDetectionGuardrail(sensitivity=Sensitivity.HIGH, indicators=[])
    text_high = guardrail_high._get_vulnerability_sensitivity_text()
    assert "even if subtle" in text_high

# --- Tests for the check method ---

@pytest.fixture
def mock_driver():
    driver = MagicMock()
    driver.send = MagicMock() # Mock the send method specifically
    return driver

@pytest.fixture
def guardrail_for_check(mock_driver):
    """Fixture to create a guardrail instance with a bound mock driver."""
    guard = VulnerabilityDetectionGuardrail(
        sensitivity=Sensitivity.MEDIUM,
        indicators=[VulnerabilityIndicators.LIFE_EVENTS],
        reasoning=True,
        confidence=True
    )
    guard.bind(mock_driver) # Bind the mock driver
    return guard

def test_check_no_driver_bound():
    """Tests that check raises RuntimeError if no driver is bound."""
    guard = VulnerabilityDetectionGuardrail(sensitivity=Sensitivity.LOW, indicators=[])
    conversation = Conversation(messages=[Message(role="user", content="Test")])
    with pytest.raises(RuntimeError, match="LLM driver not bound"):
        guard.check(conversation)

@patch("psysafe.catalog.vulnerability_detection.guardrail.parse_llm_response")
def test_check_successful_parsing_direct_json(mock_parse_llm_response, guardrail_for_check, mock_driver):
    """Tests successful parsing of direct JSON from LLM."""
    raw_llm_output_str = '{"vulnerability_detected": true, "severity_score": 0.8, "cwe_id": "CWE-123", "explanation": "High risk detected.", "reasoning": "Detailed reasons.", "confidence": 0.9}'
    parsed_llm_output = json.loads(raw_llm_output_str)
    
    mock_driver.send.return_value = {
        "choices": [{"message": {"content": raw_llm_output_str}}]
    }
    mock_parse_llm_response.return_value = parsed_llm_output

    conversation = Conversation(messages=[Message(role="user", content="I feel very vulnerable.")])
    output = guardrail_for_check.check(conversation)

    assert output.is_triggered is True
    assert output.risk_score == 0.8
    assert output.details["cwe_id"] == "CWE-123"
    assert output.details["explanation"] == "High risk detected."
    assert output.details["reasoning"] == "Detailed reasons."
    assert output.details["confidence"] == 0.9 # Check if confidence is correctly extracted
    assert output.raw_llm_response == raw_llm_output_str
    assert not output.errors
    mock_parse_llm_response.assert_called_once_with(raw_llm_output_str, logger=guardrail_for_check.logger)

@patch("psysafe.catalog.vulnerability_detection.guardrail.parse_llm_response")
def test_check_successful_parsing_json_in_markdown(mock_parse_llm_response, guardrail_for_check, mock_driver):
    """Tests successful parsing of JSON within a Markdown code block."""
    raw_llm_output_str = '```json\n{"vulnerability_detected": false, "severity_score": null, "cwe_id": null, "explanation": "No risk."}\n```'
    # parse_llm_response should handle the extraction from markdown
    parsed_llm_output = {"vulnerability_detected": False, "severity_score": None, "cwe_id": None, "explanation": "No risk."}

    mock_driver.send.return_value = {
        "choices": [{"message": {"content": raw_llm_output_str}}]
    }
    mock_parse_llm_response.return_value = parsed_llm_output

    conversation = Conversation(messages=[Message(role="user", content="I am fine.")])
    output = guardrail_for_check.check(conversation)

    assert output.is_triggered is False
    assert output.risk_score is None
    assert output.details["explanation"] == "No risk."
    assert "cwe_id" not in output.details # Should be filtered if None
    assert output.raw_llm_response == raw_llm_output_str
    assert not output.errors
    mock_parse_llm_response.assert_called_once_with(raw_llm_output_str, logger=guardrail_for_check.logger)

@patch("psysafe.catalog.vulnerability_detection.guardrail.parse_llm_response")
def test_check_llm_response_parse_error(mock_parse_llm_response, guardrail_for_check, mock_driver):
    """Tests handling of LLMResponseParseError."""
    raw_malformed_response = "This is not JSON."
    mock_driver.send.return_value = {
        "choices": [{"message": {"content": raw_malformed_response}}]
    }
    mock_parse_llm_response.side_effect = LLMResponseParseError("Malformed JSON", raw_response=raw_malformed_response)

    conversation = Conversation(messages=[Message(role="user", content="Gibberish input.")])
    output = guardrail_for_check.check(conversation)

    assert output.is_triggered is False
    assert output.risk_score is None
    assert output.raw_llm_response == raw_malformed_response
    assert len(output.errors) == 1
    assert "Failed to parse LLM response: Malformed JSON" in output.errors[0]
    assert "parser_error_type" in output.metadata
    assert output.metadata["parser_error_type"] == "LLMResponseParseError"

def test_check_llm_call_exception(guardrail_for_check, mock_driver):
    """Tests handling of exceptions during the LLM call."""
    mock_driver.send.side_effect = Exception("Network error")

    conversation = Conversation(messages=[Message(role="user", content="Test")])
    output = guardrail_for_check.check(conversation)

    assert output.is_triggered is False
    assert output.risk_score is None
    assert "Error during LLM call: Network error" in output.errors
    assert output.raw_llm_response == "Network error" # Exception string

@patch("psysafe.catalog.vulnerability_detection.guardrail.parse_llm_response")
def test_check_correct_interpretation_of_parsed_data(mock_parse_llm_response, guardrail_for_check, mock_driver):
    """Tests correct interpretation of various valid parsed data structures."""
    # Case 1: Vulnerability detected, all fields present
    raw_llm_1 = '{"vulnerability_detected": true, "severity_score": 0.95, "cwe_id": "CWE-XYZ", "explanation": "Severe issue."}'
    parsed_1 = json.loads(raw_llm_1)
    mock_driver.send.return_value = {"choices": [{"message": {"content": raw_llm_1}}]}
    mock_parse_llm_response.return_value = parsed_1
    
    output1 = guardrail_for_check.check(Conversation(messages=[Message(role="user", content="Help!")]))
    assert output1.is_triggered is True
    assert output1.risk_score == 0.95
    assert output1.details["cwe_id"] == "CWE-XYZ"
    assert output1.details["explanation"] == "Severe issue."

    # Case 2: Vulnerability not detected
    raw_llm_2 = '{"vulnerability_detected": false, "severity_score": null, "cwe_id": null, "explanation": "All clear."}'
    parsed_2 = json.loads(raw_llm_2)
    mock_driver.send.return_value = {"choices": [{"message": {"content": raw_llm_2}}]}
    mock_parse_llm_response.return_value = parsed_2

    output2 = guardrail_for_check.check(Conversation(messages=[Message(role="user", content="Okay.")]))
    assert output2.is_triggered is False
    assert output2.risk_score is None
    assert "cwe_id" not in output2.details # None value filtered
    assert output2.details["explanation"] == "All clear."

    # Case 3: Vulnerability detected as string "true", severity as string
    raw_llm_3 = '{"vulnerability_detected": "true", "severity_score": "0.5", "explanation": "Moderate."}'
    parsed_3 = json.loads(raw_llm_3) # parse_llm_response would return this dict
    mock_driver.send.return_value = {"choices": [{"message": {"content": raw_llm_3}}]}
    mock_parse_llm_response.return_value = parsed_3

    output3 = guardrail_for_check.check(Conversation(messages=[Message(role="user", content="Concerned.")]))
    assert output3.is_triggered is True
    assert output3.risk_score == 0.5
    assert output3.details["explanation"] == "Moderate."
    assert not output3.errors # Should parse "true" and "0.5" correctly

    # Case 4: Missing optional fields (cwe_id)
    raw_llm_4 = '{"vulnerability_detected": true, "severity_score": 0.7, "explanation": "Risky."}'
    parsed_4 = json.loads(raw_llm_4)
    mock_driver.send.return_value = {"choices": [{"message": {"content": raw_llm_4}}]}
    mock_parse_llm_response.return_value = parsed_4

    output4 = guardrail_for_check.check(Conversation(messages=[Message(role="user", content="Unsure.")]))
    assert output4.is_triggered is True
    assert output4.risk_score == 0.7
    assert "cwe_id" not in output4.details
    assert output4.details["explanation"] == "Risky."

    # Case 5: Invalid severity_score type
    raw_llm_5 = '{"vulnerability_detected": true, "severity_score": "high", "explanation": "Problem."}'
    parsed_5 = json.loads(raw_llm_5)
    mock_driver.send.return_value = {"choices": [{"message": {"content": raw_llm_5}}]}
    mock_parse_llm_response.return_value = parsed_5

    output5 = guardrail_for_check.check(Conversation(messages=[Message(role="user", content="Bad feeling.")]))
    assert output5.is_triggered is True # Still triggered based on vulnerability_detected
    assert output5.risk_score is None # Severity score parsing failed
    assert output5.details["explanation"] == "Problem."
    assert len(output5.errors) == 1
    assert "non-numeric 'severity_score' value: high" in output5.errors[0]

    # Case 6: Invalid vulnerability_detected type (not bool or "true"/"false" string)
    raw_llm_6 = '{"vulnerability_detected": "maybe", "severity_score": 0.1, "explanation": "Uncertain."}'
    parsed_6 = json.loads(raw_llm_6)
    mock_driver.send.return_value = {"choices": [{"message": {"content": raw_llm_6}}]}
    mock_parse_llm_response.return_value = parsed_6

    output6 = guardrail_for_check.check(Conversation(messages=[Message(role="user", content="Hmm.")]))
    assert output6.is_triggered is False # Defaults to False on invalid type
    assert output6.risk_score == 0.1
    assert output6.details["explanation"] == "Uncertain."
    assert len(output6.errors) == 1
    assert "invalid 'vulnerability_detected' value: maybe" in output6.errors[0]

@patch("psysafe.catalog.vulnerability_detection.guardrail.parse_llm_response")
def test_check_empty_or_no_content_from_llm(mock_parse_llm_response, guardrail_for_check, mock_driver):
    """Tests behavior when LLM returns empty content or choice structure is unexpected."""
    # Scenario 1: Empty content string
    mock_driver.send.return_value = {"choices": [{"message": {"content": ""}}]}
    
    output1 = guardrail_for_check.check(Conversation(messages=[Message(role="user", content="Test")]))
    assert output1.is_triggered is False
    assert "LLM response content was empty" in output1.errors[0]
    assert output1.raw_llm_response == ""
    mock_parse_llm_response.assert_not_called() # parse_llm_response should not be called

    # Scenario 2: Content is None
    mock_driver.send.return_value = {"choices": [{"message": {"content": None}}]}
    mock_parse_llm_response.reset_mock() # Reset mock for the next call

    output2 = guardrail_for_check.check(Conversation(messages=[Message(role="user", content="Test")]))
    assert output2.is_triggered is False
    assert "Could not extract content from LLM response" in output2.errors[0] # This error is hit first
    assert output2.raw_llm_response is None
    mock_parse_llm_response.assert_not_called()

    # Scenario 3: Malformed choices structure (e.g., no "message" key)
    mock_driver.send.return_value = {"choices": [{}]} # Missing "message"
    mock_parse_llm_response.reset_mock()

    output3 = guardrail_for_check.check(Conversation(messages=[Message(role="user", content="Test")]))
    assert output3.is_triggered is False
    assert "Could not extract content from LLM response" in output3.errors[0]
    assert output3.raw_llm_response is None
    mock_parse_llm_response.assert_not_called()

    # Scenario 4: No "choices" key
    mock_driver.send.return_value = {} # Missing "choices"
    mock_parse_llm_response.reset_mock()

    output4 = guardrail_for_check.check(Conversation(messages=[Message(role="user", content="Test")]))
    assert output4.is_triggered is False
    assert "Could not extract content from LLM response" in output4.errors[0]
    assert output4.raw_llm_response is None
    mock_parse_llm_response.assert_not_called()