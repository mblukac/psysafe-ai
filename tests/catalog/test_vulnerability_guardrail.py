# tests/catalog/test_vulnerability_guardrail.py
import pytest
import json # Added import
from unittest.mock import patch, MagicMock

from psysafe.catalog.vulnerability_detection.guardrail import (
    VulnerabilityDetectionGuardrail,
    VulnerabilityIndicators
)
from psysafe.core.types import SensitivityLevel
from psysafe.core.template import PromptTemplate
from psysafe.typing.requests import OpenAIChatRequest, OpenAIMessage
from psysafe.core.models import Conversation, Message
from psysafe.core.types import GuardrailResponse
from psysafe.core.exceptions import ResponseParsingError

@pytest.mark.parametrize("sensitivity", list(SensitivityLevel))
@pytest.mark.parametrize("indicators", list(VulnerabilityIndicators))
@pytest.mark.parametrize("reasoning", [True, False])
@pytest.mark.parametrize("confidence", [True, False])
def test_vulnerability_detection_guardrail_initialization(
    sensitivity, indicators, reasoning, confidence
):
    """
    Tests initialization of VulnerabilityDetectionGuardrail with all
    valid combinations of Sensitivity, VulnerabilityIndicators, reasoning, and confidence.
    """
    guardrail = VulnerabilityDetectionGuardrail(
        sensitivity=sensitivity,
        indicators=[indicators], # Expects a list
        reasoning=reasoning,
        confidence=confidence
    )
    assert guardrail.config.sensitivity == sensitivity
    assert guardrail.config.indicators == [indicators]
    assert guardrail.config.reasoning_enabled == reasoning
    assert guardrail.config.confidence_enabled == confidence
    assert isinstance(guardrail.template, PromptTemplate)
    assert guardrail.logger is not None # Check logger is initialized

def test_vulnerability_detection_guardrail_initialization_default_flags():
    """Tests initialization with default reasoning and confidence flags."""
    guardrail = VulnerabilityDetectionGuardrail(
        sensitivity=SensitivityLevel.LOW,
        indicators=[VulnerabilityIndicators.HEALTH_CONDITIONS]
    )
    assert guardrail.config.reasoning_enabled is True
    assert guardrail.config.confidence_enabled is False

def test_vulnerability_detection_guardrail_initialization_multiple_indicators():
    """Tests initialization with multiple vulnerability indicators."""
    indicators = [
        VulnerabilityIndicators.LIFE_EVENTS, 
        VulnerabilityIndicators.HEALTH_CONDITIONS
    ]
    guardrail = VulnerabilityDetectionGuardrail(
        sensitivity=SensitivityLevel.HIGH,
        indicators=indicators
    )
    assert guardrail.config.indicators == indicators

@patch("psysafe.catalog.vulnerability_detection.guardrail.PromptTemplate.from_file")
def test_vulnerability_detection_guardrail_apply_method(mock_from_file, mocker):
    """Tests the apply method of VulnerabilityDetectionGuardrail."""
    mock_template_instance = MagicMock(spec=PromptTemplate)
    mock_render_output = "Rendered Vulnerability Prompt"
    mock_template_instance.render.return_value = mock_render_output
    mock_from_file.return_value = mock_template_instance

    guardrail = VulnerabilityDetectionGuardrail(
        sensitivity=SensitivityLevel.MEDIUM,
        indicators=[VulnerabilityIndicators.LIFE_EVENTS],
        reasoning=True,
        confidence=True
    )

    original_request: OpenAIChatRequest = { # type: ignore
        "messages": [
            {"role": "system", "content": "Initial system message."},
            {"role": "user", "content": "User says something."},
        ],
        "model": "test-model",
        "driver_type": "openai"
    }
    
    modified_request_obj = guardrail.apply(original_request) # Renamed variable
    modified_request = modified_request_obj.modified_request # Access the dict

    mock_from_file.assert_called_once_with(
        "psysafe/catalog/vulnerability_detection/prompt.md"
    )
    
    assert mock_template_instance.render.call_count == 1
    render_call_args = mock_template_instance.render.call_args
    assert render_call_args is not None
    render_ctx = render_call_args[0][0] 

    assert render_ctx.variables["user_context"] == "User says something."
    assert "Recent life events:" in render_ctx.variables["vulnerability_indicators_text"]
    assert "strong implicit indicators" in render_ctx.variables["vulnerability_sensitivity_text"]
    assert render_ctx.variables["reasoning"] is True
    assert render_ctx.variables["confidence"] is True

    assert len(modified_request["messages"]) == 3
    assert modified_request["messages"][0].get("role") == "system"
    assert modified_request["messages"][0].get("content") == mock_render_output
    assert modified_request["messages"][1].get("role") == "system"
    assert modified_request["messages"][1].get("content") == "Initial system message."
    assert modified_request["messages"][2].get("role") == "user"
    assert modified_request["messages"][2].get("content") == "User says something."

@patch("psysafe.catalog.vulnerability_detection.guardrail.PromptTemplate.from_file")
def test_vulnerability_detection_guardrail_apply_no_initial_system_message(mock_from_file, mocker):
    """Tests apply when the original request has no system message."""
    mock_template_instance = MagicMock(spec=PromptTemplate)
    mock_render_output = "Rendered Vulnerability Prompt No System"
    mock_template_instance.render.return_value = mock_render_output
    mock_from_file.return_value = mock_template_instance

    guardrail = VulnerabilityDetectionGuardrail(
        sensitivity=SensitivityLevel.LOW,
        indicators=[VulnerabilityIndicators.RESILIENCE]
    )

    original_request: OpenAIChatRequest = { # type: ignore
        "messages": [{"role": "user", "content": "Just a user message."}],
        "model": "test-model",
        "driver_type": "openai"
    }
    modified_request_obj = guardrail.apply(original_request) # Renamed variable
    modified_request = modified_request_obj.modified_request # Access the dict

    mock_from_file.assert_called_once_with(
        "psysafe/catalog/vulnerability_detection/prompt.md"
    )
    mock_template_instance.render.assert_called_once()
    render_ctx = mock_template_instance.render.call_args[0][0]
    assert render_ctx.variables["user_context"] == "Just a user message."

    assert len(modified_request["messages"]) == 2
    assert modified_request["messages"][0].get("role") == "system"
    assert modified_request["messages"][0].get("content") == mock_render_output
    assert modified_request["messages"][1].get("role") == "user"
    assert modified_request["messages"][1].get("content") == "Just a user message."

def test_get_vulnerability_indicators_text():
    """Tests the _get_vulnerability_indicators_text helper method."""
    guardrail = VulnerabilityDetectionGuardrail(
        sensitivity=SensitivityLevel.LOW,
        indicators=[
            VulnerabilityIndicators.LIFE_EVENTS,
            VulnerabilityIndicators.HEALTH_CONDITIONS
        ]
    )
    text = guardrail._get_vulnerability_indicators_text()
    assert "Recent life events:" in text
    assert "Health conditions:" in text

def test_get_vulnerability_indicators_text_empty():
    """Tests _get_vulnerability_indicators_text with no indicators."""
    guardrail = VulnerabilityDetectionGuardrail(
        sensitivity=SensitivityLevel.LOW,
        indicators=[]
    )
    text = guardrail._get_vulnerability_indicators_text()
    assert text == ""

def test_get_vulnerability_sensitivity_text():
    """Tests the _get_vulnerability_sensitivity_text helper method."""
    guardrail_low = VulnerabilityDetectionGuardrail(sensitivity=SensitivityLevel.LOW, indicators=[])
    text_low = guardrail_low._get_vulnerability_sensitivity_text()
    assert "clear and explicit statements" in text_low

    guardrail_medium = VulnerabilityDetectionGuardrail(sensitivity=SensitivityLevel.MEDIUM, indicators=[])
    text_medium = guardrail_medium._get_vulnerability_sensitivity_text()
    assert "strong implicit indicators" in text_medium

    guardrail_high = VulnerabilityDetectionGuardrail(sensitivity=SensitivityLevel.HIGH, indicators=[])
    text_high = guardrail_high._get_vulnerability_sensitivity_text()
    assert "even if subtle" in text_high

# --- Tests for the check method ---

@pytest.fixture
def mock_driver():
    driver = MagicMock()
    driver.send = MagicMock() # Mock the send method specifically
    return driver

@pytest.fixture
def guardrail_for_check(mock_driver):
    """Fixture to create a guardrail instance with a bound mock driver."""
    guard = VulnerabilityDetectionGuardrail(
        sensitivity=SensitivityLevel.MEDIUM,
        indicators=[VulnerabilityIndicators.LIFE_EVENTS],
        reasoning=True,
        confidence=True
    )
    guard.set_driver(mock_driver) # Set the mock driver
    return guard

def test_check_no_driver_bound():
    """Tests that check raises an error if no driver is bound."""
    guard = VulnerabilityDetectionGuardrail(sensitivity=SensitivityLevel.LOW, indicators=[])
    conversation = Conversation(messages=[Message(role="user", content="Test")])
    # The new implementation should raise LLMDriverError
    from psysafe.core.exceptions import LLMDriverError
    with pytest.raises(LLMDriverError, match="No LLM driver configured"):
        guard.check(conversation)

def test_check_successful_parsing_direct_json(guardrail_for_check, mock_driver):
    """Tests successful parsing of direct JSON from LLM."""
    raw_llm_output_str = '{"is_vulnerable": true, "confidence_score": 0.8, "indicators_detected": ["life_events"], "reasoning": "High risk detected."}'
    
    mock_driver.send.return_value = {
        "choices": [{"message": {"content": raw_llm_output_str}}]
    }

    conversation = Conversation(messages=[Message(role="user", content="I feel very vulnerable.")])
    output = guardrail_for_check.check(conversation)

    assert output.is_triggered is True
    assert output.risk_score == 0.8
    assert output.details["is_vulnerable"] is True
    assert output.details["confidence_score"] == 0.8
    assert output.details["indicators_detected"] == ["life_events"]
    assert output.details["reasoning"] == "High risk detected."
    assert output.raw_llm_response == raw_llm_output_str
    assert not output.errors

def test_check_successful_parsing_json_in_markdown(guardrail_for_check, mock_driver):
    """Tests successful parsing of JSON within a Markdown code block."""
    raw_llm_output_str = '```json\n{"is_vulnerable": false, "confidence_score": null, "indicators_detected": [], "reasoning": "No risk."}\n```'

    mock_driver.send.return_value = {
        "choices": [{"message": {"content": raw_llm_output_str}}]
    }

    conversation = Conversation(messages=[Message(role="user", content="I am fine.")])
    output = guardrail_for_check.check(conversation)

    assert output.is_triggered is False
    assert output.risk_score is None
    assert output.details["is_vulnerable"] is False
    assert output.details["reasoning"] == "No risk."
    assert output.details["indicators_detected"] == []
    assert output.raw_llm_response == raw_llm_output_str
    assert not output.errors

def test_check_llm_response_parse_error(guardrail_for_check, mock_driver):
    """Tests handling of ResponseParsingError."""
    raw_malformed_response = "This is not JSON."
    mock_driver.send.return_value = {
        "choices": [{"message": {"content": raw_malformed_response}}]
    }

    conversation = Conversation(messages=[Message(role="user", content="Gibberish input.")])
    output = guardrail_for_check.check(conversation)

    assert output.is_triggered is False
    assert output.risk_score is None
    assert output.raw_llm_response == raw_malformed_response
    assert len(output.errors) == 1
    assert "Failed to parse LLM response" in output.errors[0]
    assert "error_type" in output.metadata
    assert output.metadata["error_type"] == "ResponseParsingError"

def test_check_llm_call_exception(guardrail_for_check, mock_driver):
    """Tests handling of exceptions during the LLM call."""
    mock_driver.send.side_effect = Exception("Network error")

    conversation = Conversation(messages=[Message(role="user", content="Test")])
    output = guardrail_for_check.check(conversation)

    assert output.is_triggered is False
    assert output.risk_score is None
    assert len(output.errors) == 1
    assert "LLM call failed: Network error" in output.errors[0]

def test_check_correct_interpretation_of_parsed_data(guardrail_for_check, mock_driver):
    """Tests correct interpretation of various valid parsed data structures."""
    # Case 1: Vulnerability detected, all fields present
    raw_llm_1 = '{"is_vulnerable": true, "confidence_score": 0.95, "indicators_detected": ["health_conditions"], "reasoning": "Severe issue."}'
    mock_driver.send.return_value = {"choices": [{"message": {"content": raw_llm_1}}]}
    
    output1 = guardrail_for_check.check(Conversation(messages=[Message(role="user", content="Help!")]))
    assert output1.is_triggered is True
    assert output1.risk_score == 0.95
    assert output1.details["is_vulnerable"] is True
    assert output1.details["confidence_score"] == 0.95
    assert output1.details["indicators_detected"] == ["health_conditions"]
    assert output1.details["reasoning"] == "Severe issue."

    # Case 2: Vulnerability not detected
    raw_llm_2 = '{"is_vulnerable": false, "confidence_score": null, "indicators_detected": [], "reasoning": "All clear."}'
    mock_driver.send.return_value = {"choices": [{"message": {"content": raw_llm_2}}]}

    output2 = guardrail_for_check.check(Conversation(messages=[Message(role="user", content="Okay.")]))
    assert output2.is_triggered is False
    assert output2.risk_score is None
    assert output2.details["is_vulnerable"] is False
    assert output2.details["reasoning"] == "All clear."

    # Case 3: Multiple indicators detected
    raw_llm_3 = '{"is_vulnerable": true, "confidence_score": 0.5, "indicators_detected": ["life_events", "resilience"], "reasoning": "Moderate risk."}'
    mock_driver.send.return_value = {"choices": [{"message": {"content": raw_llm_3}}]}

    output3 = guardrail_for_check.check(Conversation(messages=[Message(role="user", content="Concerned.")]))
    assert output3.is_triggered is True
    assert output3.risk_score == 0.5
    assert output3.details["indicators_detected"] == ["life_events", "resilience"]
    assert output3.details["reasoning"] == "Moderate risk."

def test_check_empty_or_no_content_from_llm(guardrail_for_check, mock_driver):
    """Tests behavior when LLM returns empty content or choice structure is unexpected."""
    # Scenario 1: Empty content string - should trigger parsing error
    mock_driver.send.return_value = {"choices": [{"message": {"content": ""}}]}
    
    output1 = guardrail_for_check.check(Conversation(messages=[Message(role="user", content="Test")]))
    assert output1.is_triggered is False
    assert len(output1.errors) == 1
    assert "Failed to parse LLM response" in output1.errors[0]
    assert output1.raw_llm_response == ""

    # Scenario 2: Content is None
    mock_driver.send.return_value = {"choices": [{"message": {"content": None}}]}

    output2 = guardrail_for_check.check(Conversation(messages=[Message(role="user", content="Test")]))
    assert output2.is_triggered is False
    assert "Could not extract content from LLM response" in output2.errors[0]
    assert output2.metadata["error_type"] == "LLMDriverError"

    # Scenario 3: Malformed choices structure (e.g., no "message" key)
    mock_driver.send.return_value = {"choices": [{}]} # Missing "message"

    output3 = guardrail_for_check.check(Conversation(messages=[Message(role="user", content="Test")]))
    assert output3.is_triggered is False
    assert "Could not extract content from LLM response" in output3.errors[0]
    assert output3.metadata["error_type"] == "LLMDriverError"

    # Scenario 4: No "choices" key
    mock_driver.send.return_value = {} # Missing "choices"

    output4 = guardrail_for_check.check(Conversation(messages=[Message(role="user", content="Test")]))
    assert output4.is_triggered is False
    assert "Could not extract content from LLM response" in output4.errors[0]
    assert output4.metadata["error_type"] == "LLMDriverError"